ğŸ“– README â€“ ReelSieve
ğŸ¯ Objectif

ReelSieve est un outil open source qui scanne ta bibliothÃ¨que de films et sÃ©ries, utilise un LLM (ex. Mistral via Ollama) pour dÃ©duire les mÃ©tadonnÃ©es correctes Ã  partir des noms de fichiers et infos techniques, puis dÃ©tecte et dÃ©place automatiquement les doublons.

ğŸ’¡ IdÃ©al si :

    Tu as des fichiers nommÃ©s de maniÃ¨re incohÃ©rente

    Les outils comme Radarr/Sonarr ont du mal Ã  reconnaÃ®tre certains mÃ©dias

    Tu veux un tri intelligent avant de confier ta collection Ã  un serveur comme Jellyfin

ğŸ›  FonctionnalitÃ©s

    Scan rÃ©cursif des rÃ©pertoires

    Extraction technique via ffprobe (rÃ©solution, codec, audio, sous-titresâ€¦)

    Identification des mÃ©dias par LLM (titre, annÃ©e, saison, Ã©pisodeâ€¦)

    DÃ©tection de doublons intelligents (mÃªme film/sÃ©rie mais noms diffÃ©rents)

    Scoring qualitÃ© (rÃ©solution, codec, audio, prÃ©fÃ©rences linguistiquesâ€¦)

    DÃ©placement des doublons vers un dossier de quarantaine

    Mode --dry-run pour simulation sans rien dÃ©placer

    Docker Compose prÃªt Ã  lâ€™emploi :

        Ollama dans le conteneur

        ou Ollama dÃ©jÃ  sur lâ€™hÃ´te

ğŸ“¦ Installation
1) RÃ©cupÃ©rer le projet

git clone https://github.com/<TON_COMPTE>/reelsieve.git
cd reelsieve

2) CrÃ©er et Ã©diter la config

cp config.example.yaml config.yaml

Exemple :

media_dirs:
  - /media/Films
  - /media/Series

quarantine_dir: /media/_doublons
database_path: data/media.db

llm:
  base_url: http://ollama:11434   # ou http://host.docker.internal:11434 si Ollama sur lâ€™hÃ´te
  model: mistral
  options: { temperature: 0.1, num_predict: 200 }

scoring:
  prefer_resolution: [2160, 1080, 720, 480]
  prefer_video_codec: [hevc, h265, av1, h264]
  bonus_audio_langs: ["fre","fra","fr"]
  quarantine_days: 14

ğŸš€ Utilisation
Option A â€“ Ollama intÃ©grÃ© dans Docker

# Lancer Ollama et tirer le modÃ¨le
docker compose -f docker-compose.ollama.yml up -d ollama
docker compose -f docker-compose.ollama.yml exec ollama ollama pull mistral

# Build
docker compose -f docker-compose.ollama.yml build

# Scan
docker compose -f docker-compose.ollama.yml run --rm app python -m app.cli scan

# Identification LLM
docker compose -f docker-compose.ollama.yml run --rm app python -m app.cli guess

# Rapport
docker compose -f docker-compose.ollama.yml run --rm app python -m app.cli report

# Simulation de dÃ©duplication
docker compose -f docker-compose.ollama.yml run --rm app python -m app.cli dedupe --dry-run

# DÃ©duplication rÃ©elle
docker compose -f docker-compose.ollama.yml run --rm app python -m app.cli dedupe --apply

Option B â€“ Ollama dÃ©jÃ  sur lâ€™hÃ´te

    Utiliser docker-compose.localollama.yml

    Dans config.yaml, mettre base_url: http://host.docker.internal:11434 (Windows/Mac) ou ajouter extra_hosts sur Linux

ğŸ“Š Exemple de sortie

[GUESS] ./Films/22 Jump Street (2014)/... -> {'type': 'movie', 'title': '22 Jump Street (2014)', 'year': 2014}
[GUESS] ./Films/22 Jump Street (2014) MULTi/... -> {'type': 'movie', 'title': '22 Jump Street (2014)', 'year': 2014}

[REPORT]
Titre                | AnnÃ©e | Chemin
-------------------- | ----- | ---------------------------------
22 Jump Street       | 2014  | ./Films/22 Jump Street (2014)/...
                     |       | ./Films/22 Jump Street (2014) MULTi/...

[DEDUP DRY-RUN]
Garde: ./Films/22 Jump Street (2014)/...
DÃ©place: ./Films/22 Jump Street (2014) MULTi/... -> /media/_doublons

âš™ï¸ PrÃ©requis

    Docker et Docker Compose

    ffmpeg/ffprobe (installÃ© dans le conteneur)

    AccÃ¨s Internet pour tÃ©lÃ©charger le modÃ¨le LLM (Mistral par dÃ©faut)

ğŸ”’ SÃ©curitÃ©

    Aucun fichier supprimÃ© automatiquement : les doublons vont dans quarantine_dir

    Tu peux tester en --dry-run avant dâ€™appliquer

ğŸ—º Roadmap (Phase 2)

    IntÃ©gration API TMDb pour fiabiliser les mÃ©tadonnÃ©es

    Interface web lÃ©gÃ¨re

    Support direct Radarr/Sonarr

    Matching multi-critÃ¨res (hash vidÃ©o + nom + durÃ©e)
